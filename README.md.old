# Inspection Dashboard - Washtenaw County Food Safety

A public dashboard for tracking and monitoring food safety compliance across Washtenaw County establishments. The dashboard displays inspection reports, violations, and compliance trends.

## Overview

This application provides transparency into food safety compliance by aggregating and displaying inspection data from Washtenaw County health departments. Users can search, filter, and explore inspection records, violations, and establishment compliance trends.

## Key Features

- **Text-Based Q&A**: Ask questions about Michigan food safety regulations and get AI-powered answers grounded in official compliance documents
- **Inspection Dashboard**: Browse all establishments with recent inspection data
- **Search & Filters**: Find establishments by name, address, severity, type, and date range
- **Analytics**: View most common violations, frequently cited establishments, and compliance trends
- **Mobile-Friendly**: Responsive design that works on all devices
- **County-Level Data**: Currently focused on Washtenaw County, with extensibility for additional counties

## Architecture

### Tech Stack

- **Frontend**: Next.js 15 (React)
- **Backend**: Next.js API Routes
- **Database**: Supabase (PostgreSQL)
- **AI**: Cohere API for Q&A
- **Data Source**: Python web scraper for county inspection reports

### Database Schema

The `establishments` table stores inspection data:

```sql
CREATE TABLE establishments (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    address TEXT NOT NULL,
    type TEXT,
    inspection_date DATE,
    severity TEXT,
    violations TEXT[],
    notes TEXT[],
    county TEXT DEFAULT 'washtenaw',
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

## Getting Started

### Prerequisites

- Node.js 20.x
- npm 10.x
- Python 3.7+ (for scraper)
- Supabase account

### Installation

```bash
# Install dependencies
npm install

# Install Python dependencies for scraper
cd inspection_scraper
pip install -r requirements.txt
cd ..
```

### Environment Variables

Create a `.env.local` file:

```env
# Supabase
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# Cohere AI (for Q&A)
COHERE_API_KEY=your_cohere_api_key

# Application
NEXT_PUBLIC_BASE_URL=http://localhost:3000
```

### Database Setup

Run the migrations in your Supabase SQL editor:

```bash
# Files in supabase/migrations/
1. 001_create_user_profiles.sql
2. 002_create_establishments.sql
```

### Running the Application

```bash
# Development server
npm run dev

# Production build
npm run build
npm start
```

Visit `http://localhost:3000` to see the dashboard.

## Data Collection

### Python Scraper

The application includes a Python scraper that collects inspection data from county websites.

```bash
# Run the scraper
cd inspection_scraper
python main.py

# This generates:
# - outputs/inspections.json
# - outputs/inspections.csv
```

### Syncing Data to Database

After running the scraper, sync the data to the database:

```bash
# Make sure the dev server is running
npm run dev

# In another terminal
npm run sync-inspections
```

This reads the scraper output and imports it into the Supabase database.

## API Endpoints

### Establishments

- `GET /api/establishments` - Get paginated list of establishments with filters
  - Query params: `search`, `severity`, `type`, `dateFrom`, `dateTo`, `county`, `page`, `limit`

### Analytics

- `GET /api/establishments/analytics` - Get analytics summary
  - Query params: `county`

### Q&A

- `POST /api/qa` - Ask questions about food safety regulations
  - Body: `{ "question": "..." }`

### Scraper Sync

- `POST /api/scraper/sync` - Sync scraper data to database
  - Body: `{ "records": [...] }`

## Features in Detail

### Inspection Dashboard

The main landing page displays:
- Search bar for finding establishments
- Filters for severity, type, and date range
- Analytics summary cards (total establishments, critical issues, etc.)
- List of establishments with their violations and notes
- Pagination for browsing large datasets

### Text-Based Q&A

Users can ask questions about Michigan food safety regulations:
- "What temperature must hot food be held at?"
- "How often should a restaurant be inspected?"
- Answers are grounded in official compliance documents stored in the database

### Analytics

The dashboard provides insights:
- Most common violations across all establishments
- Most frequently cited establishments
- Breakdown by severity (critical, high, medium, low)
- Recent inspection activity (last 30 days)

## Extending to Other Counties

The system is designed to support multiple counties. To add a new county:

1. Update `inspection_scraper/config/counties.json` with the new county's configuration
2. Create a scraper class in `inspection_scraper/scrapers/{county_name}.py`
3. Run the scraper for the new county
4. Sync the data with `county` parameter set to the new county name
5. Users can filter the dashboard by county

## Premium Features (Coming Soon)

A subscription placeholder is included for future features:
- Real-time alerts when new inspections are posted
- Custom reports and exports
- Advanced analytics and trending
- Email notifications for specific establishments

Pricing: $5-10/month (to be implemented with Stripe)

## Development

```bash
# Run linter
npm run lint

# Ingest compliance documents
npm run ingest

# Sync inspection data
npm run sync-inspections
```

## Deployment

1. Deploy to your hosting platform (Vercel, Railway, etc.)
2. Configure environment variables
3. Set up Supabase database and run migrations
4. Run the scraper and sync data
5. Set up a scheduled task (cron job) to run the scraper periodically

## Notes

- The scraper URLs in `config/counties.json` are placeholders and need to be updated with actual county inspection report URLs
- Always respect each county's robots.txt and terms of service
- Consider rate limiting and caching to avoid excessive requests

## License

This project is for educational and data transparency purposes.

## Support

For questions or issues, please open a GitHub issue.
